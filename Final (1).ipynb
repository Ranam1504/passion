{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPetuJPSb_Ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth \n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy4iMOf8cYAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_downloaded = drive.CreateFile({'id': '10yPw1zFzLAHIlQVSFPThPmbpA3Xv8zbY'})\n",
        "data_downloaded.GetContentFile('train.csv')\n",
        "data_downloaded = drive.CreateFile({'id': '1KSLxZ6Twuy50YxFoSos02nFsFNxPWj31'})\n",
        "data_downloaded.GetContentFile('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcgAvJvOcoZk",
        "colab_type": "code",
        "outputId": "fad1a4e4-726a-45e9-da23-bcb80e61705e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re, nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from  nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "import gensim\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwGDdo97cyzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train =pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMb80yoDc1sT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processTweet(tweet):\n",
        "  re.sub(\"[^a-zA-Z]\", \" \", tweet)\n",
        "  #Convert to lower case\n",
        "  tweet = tweet.lower()\n",
        "  #delete www.* or https?://* \n",
        "  tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
        "  #delete @username \n",
        "  tweet = re.sub('@[^\\s]+','',tweet)\n",
        "  #Remove additional white spaces\n",
        "  tweet = re.sub('[\\s]+', ' ', tweet)\n",
        "  #Replace #word with word\n",
        "  tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
        "  #trim\n",
        "  tweet = tweet.strip('\\'\"')\n",
        "  tweet =tweet.translate(str.maketrans(' ', ' ', string.punctuation))\n",
        "  #removing the stop words using nltk\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  tweet=word_tokenize(tweet)\n",
        "  #make a copy of the word_list\n",
        "  tweet = [w for w in tweet if not w in stop_words]\n",
        "  ps = PorterStemmer()\n",
        "  tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n",
        "  tweet = ' '.join(tweet)\n",
        "  return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o5glJFCc5L3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.TweetText=train.TweetText.apply(lambda line: processTweet(line))\n",
        "test.TweetText=test.TweetText.apply(lambda line: processTweet(line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU-jJpzcc8VH",
        "colab_type": "code",
        "outputId": "e083a596-be19-4db0-982f-502938f75b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "corpus= pd.concat([train,test],axis = 0)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG9Rr9-ZdtY-",
        "colab_type": "code",
        "outputId": "438a5fa6-16d6-42b5-85a6-64d0e95caf48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenized_tweet = corpus['TweetText'].apply(lambda x: x.split()) \n",
        "model_w2v = gensim.models.Word2Vec(\n",
        "            tokenized_tweet,\n",
        "            size=220, # desired no. of features/independent variables\n",
        "            window=6, # context window size\n",
        "            min_count=2,\n",
        "            sg = 1, # 1 for skip-gram model\n",
        "            hs = 0,\n",
        "            negative = 10, # for negative sampling\n",
        "            workers= 2,# no.of cores\n",
        "            seed = 34\n",
        "            \n",
        "            ) \n",
        "\n",
        "model_w2v.train(tokenized_tweet, total_examples= len(corpus['TweetText']), epochs=27) "
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2124747, 2374623)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFD1hVD-e1BN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_vector(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0.\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += model_w2v[word].reshape((1, size)) \n",
        "            count += 1.\n",
        "        except KeyError:                                   \n",
        "            continue\n",
        "    if count != 0:\n",
        "        vec /= count\n",
        "    return vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axoDrShXe3Gq",
        "colab_type": "code",
        "outputId": "7d7f2659-ca97-46c1-ba15-6d4141be15ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import numpy as np\n",
        "wordvec_arrays = np.zeros((len(tokenized_tweet), 220)) \n",
        "for i in range(len(tokenized_tweet)):\n",
        "    wordvec_arrays[i,:] = word_vector(tokenized_tweet.iloc[i], 220)\n",
        "    wordvec_df = pd.DataFrame(wordvec_arrays)\n",
        "wordvec_df.shape"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9135, 220)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npqaAGAlfAsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features_wordvec = wordvec_df.iloc[:6525,:]\n",
        "\n",
        "test_features_wordvec = wordvec_df.iloc[6525:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSVxfhJnfYy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=train['Label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kWP-SrtfJwX",
        "colab_type": "code",
        "outputId": "5f2d8e38-a076-4236-a2ec-094e47eaf84e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel='rbf')\n",
        "classifier.fit(train_features_wordvec, y)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
              "    shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVfUOBOffeeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction=classifier.predict(test_features_wordvec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q7SUAmcfrnC",
        "colab_type": "code",
        "outputId": "9f2b7017-a578-4c0e-c25d-29f71d7d6664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2us0ATSflah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s1 =pd.Series(prediction)\n",
        "result1 = s1.rename('Label') \n",
        "df= test.join(result1)\n",
        "df = df.drop('TweetText', axis=1)\n",
        "df.to_csv('submission5.csv',index=False)\n",
        "!cp submission5.csv drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}